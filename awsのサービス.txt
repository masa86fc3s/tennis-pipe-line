その後やりたいこと

dockerの仮想環境下で今行っているコードを実装させたい
デプロイした実感を味わいたい、簡単なアプリでもdashでもなんでも


1.シグネイトに自動的に提出してくれるスクリプトを作成する
2.AWSのsage makerというサービスを使って、今までやってきたことをそこでもできるようにする
3.amazon eventBridgeと、lamdaというサービスを使って、提出用のファイルがs3に保存された時点で、自動的にシグネイトに提出できるようにする。



提出用のファイルをパイプライン外で書く。
パイプラインのミスなのか、提出でミスしているのかを判断しやすいように、

・次に拡張するんだったら、sagemakerというawsのサービスがオススメ

s3に保存されたのが分かったら、このpythonファイルを実行させる機能
pythonはでーたを提出するコードを書いて、
・s3にデータが保存されたとき、検知する役目はAmazon EventBridgeというサービスを使う
このときに、lamdaというサービスを使って、スクリプトを動かす機能がある



teraform クラウド環境のサービスなどを、コードを書いて




 【EC2 × SageMaker の一般的な組み合わせ方】
✅ ざっくり流れ図：
コードをコピーする
【学習パイプライン（オフライン処理）】・・・SageMaker側
↓
【学習済みモデルファイル（.pkl, .joblib, etc）】・・・S3保管
↓
【APIサーバー側（オンライン処理）】・・・EC2 + FastAPIで本番推論
✅ やること一覧：
フェーズ	主担当	内容
データ前処理	SageMaker Studio または EC2	S3上のデータ加工、特徴量生成
モデル学習	SageMaker Training Job または Notebook	大規模データ向け、GPU対応
モデル保存	SageMaker → S3	学習済みモデルファイルをS3に保存
デプロイ用モデル取得	EC2	GitHub Actions or 起動時にS3からダウンロード
APIサーバーでロード	EC2 + FastAPI	/predictエンドポイントで推論API化
オンライン推論	EC2	ユーザーやアプリからリアルタイム予測

✅ なぜこうするか？
運用スタイル	理由
SageMakerで学習	安定したML学習環境、大量データ対応、MLOpsが楽
EC2でAPI運用	月額コスト安い、好きなライブラリ入れやすい、自由度高い

✅ データの流れイメージ（超重要）
diff
コードをコピーする
SageMaker側：
- 学習が終わったらS3に "model.pkl" 保存

EC2側：
- デプロイ時 (GitHub Actions or 起動スクリプト) にS3からモデルファイルをダウンロード
- FastAPIでロード → リクエスト来たら即時推論


